{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ydaqm7P4I6tf"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module02%20-%20Text%20Wrangling/Project%20-%20Build%20your%20Text%20Pre-processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pAayJMwxMk3k"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "colab_type": "code",
        "id": "JWXZ53D5MnRR",
        "outputId": "fabc7d60-7abc-401e-c67c-1e12d4b59088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textsearch in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (0.0.24)\n",
            "Requirement already satisfied: anyascii in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from textsearch) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from textsearch) (2.1.0)\n",
            "Requirement already satisfied: contractions in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: tqdm in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (4.66.5)\n",
            "Requirement already satisfied: nltk in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /Users/jshaik2452/Downloads/ur prep/.venv/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install textsearch\n",
        "!pip install contractions\n",
        "!pip install tqdm\n",
        "!pip install nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-macosx_10_9_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
            "Downloading gensim-4.3.3-cp312-cp312-macosx_10_9_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-macosx_10_9_x86_64.whl (20.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-macosx_10_9_x86_64.whl (39.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
            "Downloading wrapt-1.16.0-cp312-cp312-macosx_10_9_x86_64.whl (37 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.0.4 wrapt-1.16.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ssl\n",
        "import nltk\n",
        "\n",
        "# Disable SSL verification\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "# Now download the NLTK data you need\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v0G_9oc6ICdU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import contractions\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "nlp = spacy.load('en', parse=False, tag=False, entity=False)\n",
        "ps = nltk.porter.PorterStemmer()\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "\n",
        "def spacy_lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "\n",
        "def simple_stemming(text, stemmer=ps):\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    \n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "Aloz8FEcICdY",
        "outputId": "7ec681c7-aff6-4237-97df-20cf7cec1640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\r\n",
            "how are you doing\r\n",
            "I\tam\tdoing\tgreat\r\n",
            ":)\n"
          ]
        }
      ],
      "source": [
        "s = 'hello\\r\\nhow are you doing\\r\\nI\\tam\\tdoing\\tgreat\\r\\n:)'\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "9lvBAtV9ICdb",
        "outputId": "176f21c8-5e8f-46a5-fc21-fa548c5e3592"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello  how are you doing  I am doing great  :)'"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s.translate(s.maketrans(\"\\n\\t\\r\", \"   \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "34cnm0OkICdd"
      },
      "source": [
        "## Your Turn: Add in all the necessary functions and build your pre-processor!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_N5OiA07ICdd"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def text_pre_processor(text, html_strip=True, accented_char_removal=True, contraction_expansion=True,\n",
        "                       text_lower_case=True, text_stemming=False, text_lemmatization=True, \n",
        "                       special_char_removal=True, remove_digits=True, stopword_removal=True, \n",
        "                       stopword_list=None):\n",
        "    \n",
        "    # strip HTML\n",
        "    if html_strip:\n",
        "        text = strip_html_tags(text)\n",
        "    \n",
        "    # remove extra newlines (often might be present in really noisy text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    \n",
        "    # remove accented characters\n",
        "    if accented_char_removal:\n",
        "        text = remove_accented_chars(text)\n",
        "    \n",
        "    # expand contractions    \n",
        "    if contraction_expansion:\n",
        "        text = expand_contractions(text)\n",
        "        \n",
        "    \n",
        "    # lemmatize text\n",
        "    if text_lemmatization:\n",
        "        text = spacy_lemmatize_text(text) \n",
        "        \n",
        "    # remove special characters and\\or digits    \n",
        "    if special_char_removal:\n",
        "        # insert spaces between special characters to isolate them    \n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
        "        text = remove_special_characters(text, remove_digits=remove_digits)  \n",
        "        \n",
        "    # stem text\n",
        "    if text_stemming and not text_lemmatization:\n",
        "        text = simple_stemming(text)\n",
        "        \n",
        "    # lowercase the text    \n",
        "    if text_lower_case:\n",
        "        text = text.lower()\n",
        "        \n",
        "        \n",
        "    # remove stopwords\n",
        "    if stopword_removal:\n",
        "        text = remove_stopwords(text, is_lower_case=text_lower_case, \n",
        "                                stopwords=stopword_list)\n",
        "        \n",
        "    # remove extra whitespace\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "  \n",
        "def corpus_pre_processor(corpus):\n",
        "  norm_corpus = []\n",
        "  for doc in tqdm.tqdm(corpus):\n",
        "    norm_corpus.append(text_pre_processor(doc))\n",
        "  return norm_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBE3P53iICdg"
      },
      "source": [
        "# Test on a single document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "colab_type": "code",
        "id": "CjL4Mx9aICdg",
        "outputId": "0525fb8c-914e-4537-a7c1-97173266708b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\r\n",
            " \n",
            "              It's an amazing language which can be used for [Scripting\tWeb development\tBackend development],\r\n",
            "\r\n",
            "\n",
            "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\n",
            "\n",
            "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\n",
            "\n",
            "\n",
            "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
            "           \n"
          ]
        }
      ],
      "source": [
        "document = \"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
        "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
        "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
        "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
        "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
        "           \"\"\"\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "q67MCFxLICdj",
        "outputId": "84033398-3704-4f6a-bfb2-edd3a34b0299"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python'"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_pre_processor(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HsIpekwoICdl"
      },
      "source": [
        "# Test on a corpus of documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2S1H1zakICdo"
      },
      "outputs": [],
      "source": [
        "corpus = [\"\"\"<p>Héllo! Héllo! can you hear me! I just heard about <b>Python</b>!<br/>\\r\\n \n",
        "              It's an amazing language which can be used for [Scripting\\tWeb development\\tBackend development],\\r\\n\\r\\n\n",
        "              Information Retrieval, Natural Language Processing, Machine Learning & Artificial Intelligence!\\n\n",
        "              What are you waiting for? Go and get started.<br/> He's learning, she's learning, they've already\\n\\n\n",
        "              got a headstart! GET PYTHON 3.6 NOW!</p>\n",
        "           \"\"\",\n",
        "          \"\"\"US unveils world's most powerful supercomputer, beats China. \n",
        "             The US has unveiled the world's most powerful supercomputer \n",
        "             called 'Summit', beating the previous record-holder China's Sunway \n",
        "             TaihuLight. With a peak performance of 200,000 trillion calculations \n",
        "             per second, it is over twice as fast as Sunway TaihuLight, which is capable \n",
        "             of 93,000 trillion calculations per second. Summit has 4,608 servers, \n",
        "             which reportedly take up the size of two tennis courts.\"\"\",\n",
        "          \"\"\"The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. \n",
        "            The story began as a sequel to Tolkien's 1937 fantasy novel The Hobbit, but eventually developed into \n",
        "            a much larger work. Written in stages between 1937 and 1949, The Lord of the Rings is one of the \n",
        "            best-selling novels ever written, with over 150 million copies sold.[1]\n",
        "          \"\"\",\n",
        "          \"\"\"The title of the novel refers to the story's main antagonist, the Dark Lord Sauron,[a] \n",
        "             who had in an earlier age created the One Ring to rule the other Rings of Power as the ultimate weapon \n",
        "             in his campaign to conquer and rule all of Middle-earth. From quiet beginnings in the Shire, a hobbit \n",
        "             land not unlike the English countryside, the story ranges across Middle-earth, following the course \n",
        "             of the War of the Ring through the eyes of its characters, not only the hobbits Frodo Baggins, \n",
        "             Samwise \"Sam\" Gamgee, Meriadoc \"Merry\" Brandybuck and Peregrin \"Pippin\" Took, but also the hobbits' \n",
        "             chief allies and travelling companions: the Men, Aragorn, a Ranger of the North, and Boromir, \n",
        "             a Captain of Gondor; Gimli, a Dwarf warrior; Legolas Greenleaf, an Elven prince; and Gandalf, a wizard.\n",
        "          \"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "colab_type": "code",
        "id": "rj7oRS8hQYco",
        "outputId": "da2bc31f-b14f-4c2a-edac-6ee6c005faa1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 29.19it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
              " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court']"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_docs = corpus_pre_processor(corpus)\n",
        "norm_docs[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jZRQDfTTQpzh"
      },
      "source": [
        "# Processor with multi-threading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fyYMPuirQbj6"
      },
      "outputs": [],
      "source": [
        "from concurrent import futures\n",
        "import threading\n",
        "\n",
        "def parallel_preprocessing(idx, doc, total_docs):\n",
        "    return text_pre_processor(doc)\n",
        "\n",
        "\n",
        "def pre_process_documents_parallel(documents):\n",
        "    total_docs = len(documents)\n",
        "    docs_input = [[idx, doc, total_docs] for idx, doc in enumerate(documents)]\n",
        "    \n",
        "    ex = futures.ThreadPoolExecutor(max_workers=None)\n",
        "    print('preprocessing: starting')\n",
        "    norm_descriptions_map = ex.map(parallel_preprocessing, \n",
        "                                   [record[0] for record in docs_input],\n",
        "                                   [record[1] for record in docs_input],\n",
        "                                   [record[2] for record in docs_input])\n",
        "    norm_descriptions = list(norm_descriptions_map)\n",
        "    return norm_descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "colab_type": "code",
        "id": "HFBm2MmZRck3",
        "outputId": "fd917fea-0e31-46e5-af2d-2ccbea81a7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preprocessing: starting\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hello hello hear hear python amazing language use scripting web development backend development information retrieval natural language processing machine learning artificial intelligence wait go get start learn learn already get headstart get python',\n",
              " 'us unveil world powerful supercomputer beat china us unveil world powerful supercomputer call summit beat previous record holder china sunway taihulight peak performance trillion calculation per second twice fast sunway taihulight capable trillion calculation per second summit server reportedly take size two tennis court',\n",
              " 'lord rings epic high fantasy novel write english author scholar j r r tolkien story begin sequel tolkien fantasy novel hobbit eventually develop much large work write stage lord rings one best sell novel ever write million copy sold',\n",
              " 'title novel refer story main antagonist dark lord saurona early age create one ring rule rings power ultimate weapon campaign conquer rule middle earth quiet beginning shire hobbit land unlike english countryside story range across middle earth follow course war ring eye character hobbit frodo baggins samwise sam gamgee meriadoc merry brandybuck peregrin pippin take also hobbit chief ally travel companion men aragorn ranger north boromir captain gondor gimli dwarf warrior legolas greenleaf elven prince gandalf wizard']"
            ]
          },
          "execution_count": 12,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_docs = pre_process_documents_parallel(corpus)\n",
        "norm_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cDihxH-bZwsU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Project - Build your Text Pre-processor.ipynb",
      "provenance": [],
      "toc_visible": true,
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
